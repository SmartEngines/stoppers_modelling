{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook contains the code required to prepare IC15-Train and YVT datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, json, shutil\n",
    "import xml.etree.ElementTree as etree\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to split video into PNG frames. Assumes you have 'ffmpeg' installed\n",
    "def video_to_frames(path_to_video, path_to_frames_dir):\n",
    "    subprocess.call(['ffmpeg', '-i', path_to_video, \\\n",
    "                     os.path.join(path_to_frames_dir, '%05d.png')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads IDs of objects from ground truth text files in IC15-Train\n",
    "def load_ic15_gt_ids(path_to_gt_txt):\n",
    "    lines = []\n",
    "    with open(path_to_gt_txt, encoding='utf8') as fs:\n",
    "        lines = [x.strip() for x in list(fs)]\n",
    "    split_lines = [[y.strip().strip('\"') for y in x.split(',')] for x in lines]\n",
    "    return {obj_id : obj_gt for obj_id, obj_gt in split_lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads coordinates of text objects from XML ground truth fules of IC15-Train\n",
    "def load_ic15_gt_xml(path_to_gt_xml, ids):\n",
    "    tree = etree.parse(path_to_gt_xml)\n",
    "    root = tree.getroot()\n",
    "    objects = []\n",
    "    for frame in root:\n",
    "        frame_id = int(frame.attrib['ID'])\n",
    "        for obj in frame:\n",
    "            # ignoring objects not of our IDS\n",
    "            if obj.attrib['ID'] not in ids.keys():\n",
    "                continue\n",
    "            # ignoring mirrored objects\n",
    "            if 'Mirrored' in obj.attrib.keys():\n",
    "                if obj.attrib['Mirrored'] == 'Mirrored':\n",
    "                    continue\n",
    "            quad = []\n",
    "            for point in obj:\n",
    "                quad.append((int(point.attrib['x']), int(point.attrib['y'])))\n",
    "            objects.append((frame_id, obj.attrib['ID'], ids[obj.attrib['ID']], quad))\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits video from IC15-Train into separate text object clips\n",
    "def populate_dataset_from_ic15_video(path_to_dataset, path_to_video, path_to_gt_txt, path_to_gt_xml):\n",
    "    TEMP_DIR = 'temp_dir'\n",
    "    os.mkdir(TEMP_DIR)\n",
    "    \n",
    "    objects = load_ic15_gt_xml(path_to_gt_xml, load_ic15_gt_ids(path_to_gt_txt))\n",
    "    video_to_frames(path_to_video, TEMP_DIR)\n",
    "    frame_files = [os.path.join(TEMP_DIR, x) for x in sorted(os.listdir(TEMP_DIR))]\n",
    "    \n",
    "    for obj in objects:\n",
    "        frame_id, obj_id, gt, quad = obj\n",
    "        \n",
    "        obj_name = os.path.split(path_to_video)[-1].split('.')[0] + '_' + obj_id\n",
    "        obj_dir = os.path.join(path_to_dataset, obj_name)\n",
    "        if not os.path.exists(obj_dir):\n",
    "            os.mkdir(obj_dir)\n",
    "            with open(os.path.join(obj_dir + '.json'), 'w') as js:\n",
    "                js.write(json.dumps({'gt': gt.lower()}, indent = 2))\n",
    "        \n",
    "        frame = cv2.imread(frame_files[frame_id - 1])\n",
    "        xmin = min([p[0] for p in quad])\n",
    "        ymin = min([p[1] for p in quad])\n",
    "        xmax = max([p[0] for p in quad]) + 1\n",
    "        ymax = max([p[1] for p in quad]) + 1\n",
    "        \n",
    "        width = frame.shape[0]\n",
    "        height = frame.shape[1]\n",
    "        if xmin < 0 or ymax > width:\n",
    "            continue\n",
    "        if ymin < 0 or ymax > height:\n",
    "            continue\n",
    "        \n",
    "        cropped = frame[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        cv2.imwrite(os.path.join(obj_dir, '%05d.png' % frame_id), cropped)\n",
    "    \n",
    "    shutil.rmtree(TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits IC15-Train dataset into separate text object clips\n",
    "def populate_ic15(path_to_src, path_to_dst):\n",
    "    videos = [os.path.join(path_to_src, x) for x in sorted(os.listdir(path_to_src)) if x.endswith('mp4')]\n",
    "    txts   = [x.replace('.mp4', '_GT.txt') for x in videos]\n",
    "    xmls   = [x.replace('.mp4', '_GT.xml') for x in videos]\n",
    "    for v, t, x in zip(videos, txts, xmls):\n",
    "        print('Processing %s...' % v)\n",
    "        populate_dataset_from_ic15_video(path_to_dst, v, t, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split IC15-Train dataset into separate clips: run populate_ic15 with two parameters:\n",
    "#   1: path to ch3_train directory of IC15 Text in Videos dataset\n",
    "#   2: path to an output directory (should be created beforehand)\n",
    "\n",
    "# populate_ic15('/path/to/ch3_train',\\\n",
    "#               '/path/to/ic15_separate_clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads paths to YVT frames\n",
    "def load_yvt_frames(path_to_frames_dir):\n",
    "    l1dirs = [os.path.join(path_to_frames_dir, x) for x in sorted(os.listdir(path_to_frames_dir))]\n",
    "    l2dirs = []\n",
    "    for l1dir in l1dirs:\n",
    "        l2dirs.extend([os.path.join(l1dir, x) for x in sorted(os.listdir(l1dir))])\n",
    "    frame_paths = []\n",
    "    for l2dir in l2dirs:\n",
    "        frame_paths.extend([os.path.join(l2dir, x) for x in sorted(os.listdir(l2dir))])\n",
    "    return {os.path.split(f)[-1].split('.')[0] : f for f in frame_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads YVT ground truth objects\n",
    "def load_yvt_gt(path_to_gt):\n",
    "    lines = []\n",
    "    with open(path_to_gt, encoding='utf8') as fs:\n",
    "        lines = [x.strip() for x in list(fs)]\n",
    "    lines = [x.split() for x in lines]\n",
    "    objects = []\n",
    "    for line in lines:\n",
    "        # if annotation is lost\n",
    "        if line[6] != '0':\n",
    "            continue\n",
    "        # if annotation is occluded\n",
    "        if line[7] != '0':\n",
    "            continue\n",
    "        frame_id = line[5]\n",
    "        xmin = int(float(line[1]) * 720 / 1280 + 0.5)\n",
    "        ymin = int(float(line[2]) * 720 / 1280 + 0.5)\n",
    "        xmax = int(float(line[3]) * 720 / 1280 + 0.5)\n",
    "        ymax = int(float(line[4]) * 720 / 1280 + 0.5)\n",
    "        obj_id = line[0]\n",
    "        gt = line[9].lower().strip('\"')\n",
    "        \n",
    "        objects.append((frame_id, obj_id, gt, xmin, ymin, xmax, ymax))\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits video from YVT into separate text object clips\n",
    "def populate_dataset_from_yvt_video(path_to_dataset, path_to_frames_dir, path_to_gt):\n",
    "\n",
    "    frame_files = load_yvt_frames(path_to_frames_dir)\n",
    "    objects = load_yvt_gt(path_to_gt)\n",
    "    \n",
    "    for obj in objects:\n",
    "        frame_id, obj_id, gt, xmin, ymin, xmax, ymax = obj\n",
    "        \n",
    "        obj_name = os.path.split(path_to_frames_dir)[-1].split('.')[0] + '_' + obj_id\n",
    "        obj_dir = os.path.join(path_to_dataset, obj_name)\n",
    "        if not os.path.exists(obj_dir):\n",
    "            os.mkdir(obj_dir)\n",
    "            with open(os.path.join(obj_dir + '.json'), 'w') as js:\n",
    "                js.write(json.dumps({'gt': gt.lower()}, indent = 2))\n",
    "        \n",
    "        frame = cv2.imread(frame_files[frame_id])\n",
    "        xmax = xmax + 1\n",
    "        ymax = ymax + 1\n",
    "        \n",
    "        width = frame.shape[0]\n",
    "        height = frame.shape[1]\n",
    "        if xmin < 0 or ymax > width:\n",
    "            continue\n",
    "        if ymin < 0 or ymax > height:\n",
    "            continue\n",
    "        \n",
    "        cropped = frame[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        cv2.imwrite(os.path.join(obj_dir, '%05d.png' % int(frame_id)), cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits YVT dataset into separate text object clips\n",
    "def populate_yvt(path_to_src, path_to_dst):\n",
    "    subsets = [os.path.join(path_to_src, 'frames', x) for x in sorted(os.listdir(os.path.join(path_to_src, 'frames')))]\n",
    "    for subset in subsets:\n",
    "        clips = [os.path.join(subset, x) for x in sorted(os.listdir(subset))]\n",
    "        annotations = [x.replace('frames', 'annotations') + '.txt' for x in clips]\n",
    "        for v, t in zip(clips, annotations):\n",
    "            if not os.path.exists(v):\n",
    "                continue\n",
    "            if not os.path.exists(t):\n",
    "                continue\n",
    "            print('Processing %s...' % v)\n",
    "            populate_dataset_from_yvt_video(path_to_dst, v, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split YVT dataset into separate clips: run populate_yvt with two parameters:\n",
    "#   1: path to the YVT dataset directory\n",
    "#   2: path to an output directory (should be created beforehand)\n",
    "\n",
    "# populate_yvt('/path/to/YVT',\\\n",
    "#              '/path/to/yvt_separate_clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters objects with alphanumeric characters, splits text object clips into subclips with 30 frames\n",
    "def filter_and_convert_to_clips(path_to_src, path_to_dst):\n",
    "    clip_dirs = [os.path.join(path_to_src, x) for x in sorted(os.listdir(path_to_src)) if not x.endswith('.json')]\n",
    "    clip_gts  = [x + '.json' for x in clip_dirs]\n",
    "    for i in range(len(clip_gts)):\n",
    "        gt_data = None\n",
    "        with open(clip_gts[i]) as js:\n",
    "            gt_data = json.load(js)\n",
    "        clip_gts[i] = gt_data['gt']\n",
    "    \n",
    "    alphabet = set('0123456789abcdefghijklmnopqrstuvwxyz')\n",
    "    for clip_dir, clip_gt in zip(clip_dirs, clip_gts):\n",
    "        # discarding clip if it has non-alphabet character in it\n",
    "        discard_clip = False\n",
    "        for c in clip_gt:\n",
    "            if c not in alphabet:\n",
    "                discard_clip = True\n",
    "                break\n",
    "        if discard_clip:\n",
    "            continue\n",
    "            \n",
    "        frames_list = [os.path.join(clip_dir, x) for x in sorted(os.listdir(clip_dir))]\n",
    "        \n",
    "        # discarding clip if it has fewer than 30 frames\n",
    "        if len(frames_list) < 30:\n",
    "            continue\n",
    "        \n",
    "        for i_subclip in range(len(frames_list) // 30):\n",
    "            subclip_name = os.path.split(clip_dir)[-1] + '__%02d___%s' % (i_subclip, clip_gt)\n",
    "            subclip_dir  = os.path.join(path_to_dst, subclip_name)\n",
    "            os.mkdir(subclip_dir)\n",
    "            for i_frame in range(i_subclip * 30, i_subclip * 30 + 30):\n",
    "                shutil.copy(frames_list[i_frame], os.path.join(subclip_dir, os.path.split(frames_list[i_frame])[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert text object clips into filtered truncated clips, call 'filter_and_convert_to_clips' twice with two parameters:\n",
    "#  1: Path to split clips, prepared in the previous steps\n",
    "#  2: Path to output directory (which should be created beforehand)\n",
    "\n",
    "# filter_and_convert_to_clips('/path/to/ic15_separate_clips',\\\n",
    "#                             '/path/to/ic15_filtered_clips')\n",
    "# filter_and_convert_to_clips('/path/to/yvt_separate_clips',\\\n",
    "#                             '/path/to/yvt_filtered_clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function launches recognition of each clip using clovaai model (https://github.com/clovaai/deep-text-recognition-benchmark),\n",
    "# extracts character membership estimations, and converts the dataset to a set of pickled files which work as an \n",
    "# input for stopping rules evaluation.\n",
    "\n",
    "# To run this, you need to do some preparations:\n",
    "# 1. Clone the git repository for clovaai model (https://github.com/clovaai/deep-text-recognition-benchmark)\n",
    "# 2. Install all dependencies as per the instructions in the clovaai github's README\n",
    "# 3. Download the pretrained model which will be used for recognition. We used TPS-ResNet-BiLSTM-Attn.pth. Link \n",
    "#    for downloading the pretrained model is in the clovaai github's README\n",
    "# 4. Create directories 'images' and 'results' in the directory with cloned repo\n",
    "# 5. Some modifications need to be made in the demonstration code, in order to extract class membership estimations (all changes are in demo.py):\n",
    "\n",
    "# diff --git a/demo.py b/demo.py\n",
    "# index 45f3d04..b5fe9f6 100755\n",
    "# --- a/demo.py\n",
    "# +++ b/demo.py\n",
    "# @@ -1,5 +1,6 @@\n",
    "#  import string\n",
    "#  import argparse\n",
    "# +import pickle\n",
    " \n",
    "#  import torch\n",
    "#  import torch.backends.cudnn as cudnn\n",
    "# @@ -11,6 +12,16 @@ from dataset import RawDataset, AlignCollate\n",
    "#  from model import Model\n",
    "#  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "# +ALPHABET='$^0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "# +def decode_character(prob):\n",
    "# +    _, index = prob.max(dim=0)\n",
    "# +    return ALPHABET[index]\n",
    "# +\n",
    "# +def make_ocrcell(prob):\n",
    "# +    pruned = [float(x) for x in prob[2:]]\n",
    "# +    s = sum(pruned)\n",
    "# +    normalized = [x / s for x in pruned]\n",
    "# +    return {c : p for c, p in zip(ALPHABET[2:], normalized)}\n",
    " \n",
    "#  def demo(opt):\n",
    "#      \"\"\" model configuration \"\"\"\n",
    "# @@ -77,7 +88,7 @@ def demo(opt):\n",
    " \n",
    "#              preds_prob = F.softmax(preds, dim=2)\n",
    "#              preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "# -            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "# +            for img_name, pred, pred_max_prob, pred_prob in zip(image_path_list, preds_str, preds_max_prob, preds_prob):\n",
    "#                  if 'Attn' in opt.Prediction:\n",
    "#                      pred_EOS = pred.find('[s]')\n",
    "#                      pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "# @@ -86,6 +97,16 @@ def demo(opt):\n",
    "#                  # calculate confidence score (= multiply of pred_max_prob)\n",
    "#                  confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    " \n",
    "# +                ocr_cells = []\n",
    "# +                for pred_prob_char in pred_prob:\n",
    "# +                    if decode_character(pred_prob_char) == '^':\n",
    "# +                        break\n",
    "# +                    ocr_cells.append(make_ocrcell(pred_prob_char))\n",
    "# +                res_name = img_name.replace('images', 'results') + '.pkl'\n",
    "# +                with open(res_name, 'wb') as ps:\n",
    "# +                    pickle.dump(ocr_cells, ps)\n",
    "# +\n",
    "# +\n",
    "#                  print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')\n",
    "#                  log.write(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\\n')\n",
    "\n",
    "def recognize_and_convert_to_final_dataset(path_to_clips, path_to_final_dataset):\n",
    "    # These paths should be defined and un-commented\n",
    "#     PATH_TO_MODEL_DEMO = '/path/to/cloned/repo/deep-text-recognition-benchmark/demo.py'\n",
    "#     PATH_TO_MODEL_FILE = '/path/to/downloaded/pretrained/model/TPS-ResNet-BiLSTM-Attn.pth'\n",
    "#     PATH_TO_MODEL_INPUT = '/path/to/cloned/repo/deep-text-recognition-benchmark/images'\n",
    "#     PATH_TO_MODEL_OUTPUT = '/path/to/cloned/repo/deep-text-recognition-benchmark/results'\n",
    "    \n",
    "    pkls_path = os.path.join(path_to_final_dataset, 'none')\n",
    "    os.mkdir(pkls_path)\n",
    "    \n",
    "    input_clips = [os.path.join(path_to_clips, x) for x in sorted(os.listdir(path_to_clips))]\n",
    "    for input_clip in input_clips:\n",
    "        clip_id = os.path.split(input_clip)[-1]\n",
    "        ideal = clip_id.split('___')[-1]\n",
    "        # cleaning up model input & output\n",
    "        input_items = [os.path.join(PATH_TO_MODEL_INPUT, x) for x in sorted(os.listdir(PATH_TO_MODEL_INPUT))]\n",
    "        output_items = [os.path.join(PATH_TO_MODEL_OUTPUT, x) for x in sorted(os.listdir(PATH_TO_MODEL_OUTPUT))]\n",
    "        for item in input_items:\n",
    "            os.remove(item)\n",
    "        for item in output_items:\n",
    "            os.remove(item)\n",
    "        # copying input\n",
    "        input_frames = [os.path.join(input_clip, x) for x in sorted(os.listdir(input_clip))]\n",
    "        for input_frame in input_frames:\n",
    "            shutil.copy(input_frame, os.path.join(PATH_TO_MODEL_INPUT, os.path.split(input_frame)[-1]))\n",
    "        # running recognition\n",
    "        subprocess.call([\n",
    "            'python', \n",
    "            PATH_TO_MODEL_DEMO,\n",
    "            '--Transformation',\n",
    "            'TPS',\n",
    "            '--FeatureExtraction',\n",
    "            'ResNet',\n",
    "            '--SequenceModeling',\n",
    "            'BiLSTM',\n",
    "            '--Prediction',\n",
    "            'Attn',\n",
    "            '--image_folder',\n",
    "            PATH_TO_MODEL_INPUT,\n",
    "            '--saved_model',\n",
    "            PATH_TO_MODEL_FILE\n",
    "        ])\n",
    "        # gathering results\n",
    "        output_string_pkls = [os.path.join(PATH_TO_MODEL_OUTPUT, x) for x in sorted(os.listdir(PATH_TO_MODEL_OUTPUT))]\n",
    "        output_result = {\n",
    "            'clip_id': clip_id,\n",
    "            'field_name': clip_id,\n",
    "            'field_type': 'none',\n",
    "            'ideal': ideal,\n",
    "            'clip': []\n",
    "        }\n",
    "        for output_string_pkl in output_string_pkls:\n",
    "            output_string = None\n",
    "            with open(output_string_pkl, 'rb') as ps:\n",
    "                output_string = pickle.load(ps)\n",
    "            output_result['clip'].append(output_string)\n",
    "        if len(output_result['clip']) == 0:\n",
    "            continue\n",
    "        with open(os.path.join(pkls_path, clip_id + '.pkl'), 'wb') as ps:\n",
    "            pickle.dump(output_result, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, now to convert the filtered truncated clips into final datasets for analysis, \n",
    "#   'recognize_and_convert_to_final_dataset' should be called with two parameters:\n",
    "#  1: Path to filtered clips\n",
    "#  2: Path to output directory (which should be created beforehand)\n",
    "\n",
    "# recognize_and_convert_to_final_dataset('/path/to/ic15_filtered_clips',\\\n",
    "#                                        '/path/to/data_ic15')\n",
    "# recognize_and_convert_to_final_dataset('/path/to/yvt_filtered_clips',\\\n",
    "#                                        '/path/to/data_yvt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
